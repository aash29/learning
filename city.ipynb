{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "#import numpy as np\n",
    "import array\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "ENV_NAME = 'city'\n",
    "\n",
    "distToWork = 5;\n",
    "distToShop = 2;\n",
    "workDifficulty = 1;\n",
    "workTemperature = 1;\n",
    "workStart = 0;\n",
    "workDuration = 24;\n",
    "timeLimit = 90;\n",
    "\n",
    "\n",
    "class eat():\n",
    "    duration = 1\n",
    "    name = 'eat'\n",
    "\n",
    "    # @staticmethod\n",
    "    def pre(state):\n",
    "        # result = super(eat,eat).pre();\n",
    "        result = (state[10] < timeLimit) & (state[5] > 0) & (state[0] > 0) & (state[9] == 0) & (state[4] > 0)\n",
    "        return result\n",
    "\n",
    "    # @staticmethod\n",
    "    def eff(state):\n",
    "        # global utime\n",
    "        # super(eat,eat).eff();\n",
    "\n",
    "        s1 = list(deepcopy(state))\n",
    "\n",
    "        s1[10] += eat.duration\n",
    "        s1[0] = 32\n",
    "        s1[4] -= 1\n",
    "        s1[5] -= eat.duration\n",
    "\n",
    "        return tuple(s1)\n",
    "\n",
    "class wait():\n",
    "    name = 'wait'\n",
    "    duration = 5\n",
    "    def __str__(self):\n",
    "        return 'wait'\n",
    "    def pre(state):\n",
    "        return (state[10] < timeLimit) & (state[5] > 0) & (state[0] > 0)\n",
    "    def eff(state):\n",
    "        s1 = list(deepcopy(state))\n",
    "        s1[0] -= wait.duration\n",
    "        s1[5] += 20\n",
    "        s1[10] += wait.duration\n",
    "\n",
    "        \n",
    "        #print('Waiting');\n",
    "        return tuple(s1)\n",
    "\n",
    "class gotoWork():\n",
    "    name = 'go to work'\n",
    "    duration = 2*distToWork+1\n",
    "    def pre(state):\n",
    "        return (state[10]<timeLimit) & (state[5] > 0) & (state[0] > 0) & (state[9] != 1)\n",
    "    def eff(state):\n",
    "        s1 = list(deepcopy(state))\n",
    "\n",
    "        s1[9] = 1\n",
    "\n",
    "        s1[0] -= 1\n",
    "\n",
    "        s1[5] -= gotoWork.duration*3\n",
    "\n",
    "        s1[10] += gotoWork.duration\n",
    "\n",
    "        return tuple(s1)\n",
    "\n",
    "\n",
    "class goToShop():\n",
    "    name = 'go to shop'\n",
    "    duration = 2 * distToShop + 1\n",
    "\n",
    "    def pre(state):\n",
    "        return (state[10] < timeLimit) & (state[5] > 0) & (state[0] > 0) & (state[9] != 2)\n",
    "\n",
    "    def eff(state):\n",
    "        s1 = list(deepcopy(state))\n",
    "\n",
    "        s1[9] = 2\n",
    "\n",
    "        s1[0] -= 1\n",
    "\n",
    "        s1[5] -= goToShop.duration*3\n",
    "\n",
    "        s1[10] += goToShop.duration*3\n",
    "\n",
    "        return tuple(s1)\n",
    "\n",
    "\n",
    "class buyFood():\n",
    "    name = 'buy food'\n",
    "    duration = 1\n",
    "    def pre(state):\n",
    "        return (state[10] < timeLimit) & (state[5] > 0) & (state[0] > 0) & (state[9] == 2) & (state[2] > 0)\n",
    "\n",
    "    def eff(state):\n",
    "        s1 = list(deepcopy(state))\n",
    "        s1[2] -= 1\n",
    "        s1[4] += 1\n",
    "        return tuple(s1)\n",
    "\n",
    "\n",
    "\n",
    "class goHome():\n",
    "    name = 'go to work'\n",
    "    duration = 2 * distToShop + 1\n",
    "    def pre(state):\n",
    "        return (state[10] < timeLimit) & (state[5] > 0) & (state[0] > 0) & (state[9] != 0)\n",
    "    def eff(state):\n",
    "        s1 = list(deepcopy(state))\n",
    "\n",
    "        s1[9] = 0\n",
    "\n",
    "        s1[0] -= 1\n",
    "\n",
    "        s1[5] -= goHome.duration*3\n",
    "\n",
    "        s1[10] += goHome.duration\n",
    "\n",
    "        return tuple(s1)\n",
    "\n",
    "\n",
    "\n",
    "class work():\n",
    "    name = 'work'\n",
    "    duration = 20\n",
    "    def pre(state):\n",
    "        return (state[10] < timeLimit) & (state[5] > 0) & (state[0] > 0) & (state[9] == 1)\n",
    "    def eff(state):\n",
    "        s1 = list(deepcopy(state))\n",
    "\n",
    "        s1[5] -= work.duration*3\n",
    "\n",
    "        s1[0] -= 1\n",
    "\n",
    "        s1[7] += 1\n",
    "        s1[10] += work.duration\n",
    "\n",
    "        return tuple(s1)\n",
    "\n",
    "actions = [eat, wait, gotoWork, work, goHome, goToShop, buyFood]\n",
    "\n",
    "class city(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.distToWork = 1;\n",
    "        self.distToShop = 1;\n",
    "        self.workDifficulty = 1;\n",
    "        self.workTemperature = 1;\n",
    "        self.workStart = 0;\n",
    "        self.workDuration = 24;\n",
    "        self.timeLimit = 90;\n",
    "\n",
    "        #0: sat = 32;\n",
    "        #1: heatHome = 0;\n",
    "        #2: inv = # foodstamp;\n",
    "        #3: invHomeWood = 0;\n",
    "        #4: invHomeFood = 0;\n",
    "        #5: energy = 256;\n",
    "        #6: absent = 0;\n",
    "        #7: workDays = 0;\n",
    "        #8: isHomeHeated = 0;\n",
    "        #9: loc  # atHome = 0, atWork = 1, atShop = 2\n",
    "        #10: utime = 0;  # universal time\n",
    "\n",
    "\n",
    "        #self.action_space = gym.spaces.Discrete(4);\n",
    "        #self.observation_space = gym.spaces.Box(low=np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),high=np.array([32, 10, 10, 10, 10, 256, 3, 5, 1, 2, 96]));\n",
    "\n",
    "        self.isd = (20,\n",
    "                             0,\n",
    "                             2,\n",
    "                             1,\n",
    "                             1,\n",
    "                             256,\n",
    "                             0,\n",
    "                             0,\n",
    "                             0,\n",
    "                             0,\n",
    "                             0)\n",
    "        self.s = deepcopy(self.isd)\n",
    "        self.nActions = 0\n",
    "        self.observation = deepcopy(self.isd)\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, a):\n",
    "        action = a\n",
    "        r = 0\n",
    "        s1 = action.eff(self.s)\n",
    "\n",
    "        #if (action.pre(self.s)):\n",
    "        #    s1 = action.eff(self.s)\n",
    "        #else:\n",
    "        #    r -= 100\n",
    "\n",
    "\n",
    "        if (s1[10] > 70) & (s1[10] < 100) & (s1[5] > 0) & (s1[0] > 0) & (s1[7] > 1):\n",
    "            r += 100\n",
    "            r += s1[0]*10 #sat\n",
    "            r += s1[4]*10  #food\n",
    "            if (s1[9] == 0):\n",
    "                r += 100\n",
    "            r += s1[7]*10 #work done\n",
    "\n",
    "        self.s = s1\n",
    "        self.nActions += 1\n",
    "\n",
    "        r += s1[0] * 1  # sat\n",
    "        r += s1[7] * 1  # work done\n",
    "\n",
    "        if self.nActions > 100:\n",
    "            reset = True\n",
    "        else:\n",
    "            reset = False\n",
    "        return s1, r, reset, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.s = deepcopy(self.isd)\n",
    "        self.nActions = 0\n",
    "        return self.s\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        print(self.s)\n",
    "\n",
    "\n",
    "\n",
    "def getLegalActions(s):\n",
    "    legalActions = []\n",
    "    for action in actions:\n",
    "        if (action.pre(s)):\n",
    "            legalActions.append(action)\n",
    "    return legalActions\n",
    "    \n",
    "\n",
    "def play_and_train(env,agent,t_max=10**4):\n",
    "    \"\"\"\n",
    "    This function should \n",
    "    - run a full game, actions given by agent's e-greedy policy\n",
    "    - train agent using agent.update(...) whenever it is possible\n",
    "    - return total reward\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    s = env.reset()\n",
    "    \n",
    "    \n",
    "    for t in range(t_max):\n",
    "        # get agent to pick action given state s.\n",
    "        a = agent.get_action(s)\n",
    "        #print(a)\n",
    "        if a == None:\n",
    "            break\n",
    "        \n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        #print(\"step\")\n",
    "        \n",
    "        # train (update) agent for state s\n",
    "        agent.update(s,a, r,next_s)\n",
    "        \n",
    "        s = next_s\n",
    "        total_reward +=r\n",
    "        if done: break\n",
    "        \n",
    "    return total_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlearning import QLearningAgent\n",
    "agent = QLearningAgent(alpha=0.5, epsilon=0.5, discount=0.99,\n",
    "                       get_legal_actions = getLegalActions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-79a083a02b52>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-79a083a02b52>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    fig.\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Get the environment and extract the number of actions.\n",
    "#env = gym.make(ENV_NAME)\n",
    "env = city();\n",
    "np.random.seed(123)\n",
    "#env.seed(123)\n",
    "#nb_actions = env.action_space.n\n",
    "\n",
    "\n",
    "\n",
    "rewards = []\n",
    "for i in range(10000):\n",
    "    rewards.append(play_and_train(env, agent))\n",
    "    agent.epsilon *= 0.999\n",
    "    \n",
    "    if i %1000 ==0:\n",
    "        #clear_output(True)\n",
    "        print('eps =', agent.epsilon, 'mean reward =', np.mean(rewards[-10:]))\n",
    "        fig = plt.figure(1)\n",
    "        plt.plot(rewards)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "s0 = env.isd;\n",
    "while (s0[10]<timeLimit):\n",
    "    a0 = agent.get_action(s0)\n",
    "    s0 = a0.eff(s0)\n",
    "    print(a0)\n",
    "    print(s0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
